# -*- coding: utf-8 -*-
"""FINAL_PROJECT_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hv5Lq9oIrxNBS9nvhSW3DRrnSWMrX4U_
"""

# =========================
# 1. Imports & Setup
# =========================
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings("ignore")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =========================
# 2. Utility Metrics
# =========================
def mase(y_true, y_pred):
    naive = np.mean(np.abs(np.diff(y_true)))
    return np.mean(np.abs(y_true - y_pred)) / naive

def smape(y_true, y_pred):
    return 100 * np.mean(
        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    )

# =========================
# 3. Load Dataset
# =========================
df = pd.read_csv("/content/household_power_consumption.csv")
df.columns = df.columns.str.strip().str.lower()
df["time"] = pd.to_datetime(df["time"], errors="coerce")
df = df.set_index("time")
df = df.apply(pd.to_numeric, errors="coerce")
df = df.interpolate(method="time").dropna()

# =========================
# 4. Feature Engineering
# =========================
df["hour"] = df.index.hour
df["dayofweek"] = df.index.dayofweek
df["rolling_mean_24"] = df["global_active_power"].rolling(24).mean()
df["rolling_std_24"] = df["global_active_power"].rolling(24).std()
df = df.dropna()

features = [
    "global_active_power",
    "global_reactive_power",
    "voltage",
    "global_intensity",
    "sub_metering_1",
    "sub_metering_2",
    "sub_metering_3",
    "hour",
    "dayofweek",
    "rolling_mean_24",
    "rolling_std_24"
]

# =========================
# 5. Scaling
# =========================
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[features])

# =========================
# 6. Sequence Creation
# =========================
def create_sequences(data, seq_len, target_col=0):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, target_col])
    return np.array(X), np.array(y)

SEQ_LEN = 48
X, y = create_sequences(scaled_data, SEQ_LEN)

split = int(0.8 * len(X))
X_train, X_val = X[:split], X[split:]
y_train, y_val = y[:split], y[split:]

# =========================
# 7. Dataset
# =========================
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(
    TimeSeriesDataset(X_train, y_train),
    batch_size=64,
    shuffle=True
)

# =========================
# 8. Transformer Model
# =========================
class TransformerForecast(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers):
        super().__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            self.encoder_layer,
            num_layers=num_layers
        )
        self.fc = nn.Linear(d_model, 1)
        self.attention_weights = None

    def forward(self, x):
        x = self.embedding(x)
        attn_out, attn_weights = self.encoder_layer.self_attn(x, x, x)
        self.attention_weights = attn_weights
        x = self.transformer(attn_out)
        return self.fc(x[:, -1, :]).squeeze()

# =========================
# 9. Train Transformer
# =========================
model = TransformerForecast(
    input_dim=X.shape[2],
    d_model=64,
    nhead=4,
    num_layers=2
).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

model.train()
for epoch in range(2):
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        loss = criterion(model(xb), yb)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

# =========================
# 10. Transformer Metrics
# =========================
model.eval()
with torch.no_grad():
    preds_tr = model(torch.tensor(X_val, dtype=torch.float32).to(device)).cpu().numpy()

rmse_tr = np.sqrt(mean_squared_error(y_val, preds_tr))
mase_tr = mase(y_val, preds_tr)
smape_tr = smape(y_val, preds_tr)

# =========================
# 11. Attention Interpretation
# =========================
with torch.no_grad():
    _ = model(torch.tensor(X_val[:1], dtype=torch.float32).to(device))
    attention = model.attention_weights.mean(dim=1).cpu().numpy()
    timestep_importance = attention.mean(axis=0)

top_steps = np.argsort(timestep_importance)[-5:]

print("\nTop influential timesteps (relative to 48-step window):", top_steps)

# =========================
# 12. XGBoost Baseline
# =========================
lag_df = pd.DataFrame(index=df.index)
for col in features:
    lag_df[f"{col}_lag1"] = df[col].shift(1)
    lag_df[f"{col}_lag24"] = df[col].shift(24)

lag_df = lag_df.dropna()
target = df.loc[lag_df.index, "global_active_power"]

X_lag = lag_df.values
y_lag = target.values

split2 = int(0.8 * len(X_lag))
X_train_xgb, X_test_xgb = X_lag[:split2], X_lag[split2:]
y_train_xgb, y_test_xgb = y_lag[:split2], y_lag[split2:]

xgb = XGBRegressor(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    objective="reg:squarederror"
)

xgb.fit(X_train_xgb, y_train_xgb)
preds_xgb = xgb.predict(X_test_xgb)

rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, preds_xgb))
mase_xgb = mase(y_test_xgb, preds_xgb)
smape_xgb = smape(y_test_xgb, preds_xgb)

# =========================
# 13. Final Metrics Table
# =========================
results = pd.DataFrame({
    "Model": ["Transformer", "XGBoost"],
    "RMSE": [rmse_tr, rmse_xgb],
    "MASE": [mase_tr, mase_xgb],
    "sMAPE (%)": [smape_tr, smape_xgb]
})

print("\nFinal Performance Comparison:")
print(results)

results.to_csv("final_metrics.csv", index=False)

# =========================
# 14. Save Model
# =========================
torch.save(model.state_dict(), "final_transformer_attention_model.pth")